{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dd3062e-6d6d-4bf9-9430-1528f2eb643a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.dsl import component, pipeline\n",
    "from kfp.v2.dsl import Dataset, Output, Input, Metrics, Markdown, Artifact\n",
    "from kfp.v2 import compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9ef4e92-a4e8-451e-941f-a6d1941327c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Component 1: Load images from given directory and parse them into JSON\n",
    "@component()\n",
    "def parse_images_from_bucket(\n",
    "    bucket_name: str, \n",
    "    image_dir: str,\n",
    "    output_json_file: Output[Artifact]\n",
    "):\n",
    "    # Import Modules (This is required for each of the component function)\n",
    "    from google.cloud import storage\n",
    "    import base64\n",
    "    import json\n",
    "    \n",
    "    # Initialize a client\n",
    "    storage_client = storage.Client()\n",
    "    # Create a bucket object\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    # Create blob objects from the filepath\n",
    "    blobs = bucket.list_blobs(prefix=image_dir)\n",
    "    # Iterate over the blobs and filter based on file extension\n",
    "    image_extensions = ('.png', '.jpg', '.jpeg')\n",
    "    image_blobs = [blob for blob in blobs if blob.name.lower().endswith(image_extensions)]\n",
    "    \n",
    "    # Download all images in String\n",
    "    image_base64_dict = {\"instances\": []}\n",
    "    for image_blob in image_blobs:\n",
    "        base64_str = base64.b64encode(image_blob.download_as_bytes()).decode('utf-8')\n",
    "        # image_base64_dict[image_blob.name] = base64_str\n",
    "        image_base64_dict[\"instances\"].append({\n",
    "            'image': base64_str,\n",
    "            'file_path': f\"gs://{bucket_name}/{image_blob.name}\"\n",
    "        })\n",
    "    \n",
    "    # Save it to Artifact\n",
    "    with open(output_json_file.path, \"w\") as file:\n",
    "        json.dump(image_base64_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbac129-06dd-4429-99a7-2a6bd3bdcb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Component 2: Trigger Batch Prediction Job and Get Batch Prediction Result\n",
    "@component()\n",
    "def get_batch_prediction(\n",
    "    json_file: Input[Artifact],\n",
    "    visualization: Output[Markdown]\n",
    "):\n",
    "    # Import Modules\n",
    "    from google.cloud import storage\n",
    "    import json\n",
    "    \n",
    "    # Load JSON File\n",
    "    with open(json_file.path, 'r') as file:\n",
    "        image_json = json.load(file)\n",
    "    \n",
    "    # TODO: Get Batch Prediction Results\n",
    "    pass\n",
    "    \n",
    "    # TODO: Static Visualization\n",
    "    # Please update the Markdown file after getting the prediction result\n",
    "    with open(visualization.path, 'w') as f:\n",
    "        for image_dict in image_json['instances']:\n",
    "            f.write(f\"# {image_dict['file_path']} \\n\")\n",
    "            f.write(f\"![Image](data:image/png;base64,{image_dict['image']})\")\n",
    "    \n",
    "    # TODO: Save Metadata To BigQuery\n",
    "    # Schema: Path, Original Image, Masked Image, Number of Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8fbd19-4fd5-4406-8120-321de73052c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline Initialization\n",
    "@pipeline(\n",
    "    pipeline_root=\"gs://pipeline_artifacts\",\n",
    "    name=\"sam-pipeline\",\n",
    ")\n",
    "def sam_pipeline(\n",
    "    bucket_name: str = \"wallace-playground\",\n",
    "    image_dir: str = \"batch_1\"\n",
    "):\n",
    "    parse_image_op = parse_images_from_bucket(bucket_name, image_dir)\n",
    "\n",
    "    get_batch_prediction_op = get_batch_prediction(json_file=parse_image_op.outputs['output_json_file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a809232-2412-4726-ac1a-ed7d111252c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=sam_pipeline,\n",
    "    package_path='sam_pipe.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbe8962-3d09-4d01-87c4-66eeb56762d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp sam_pipe.json gs://segment-anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07beb5cd-658e-4637-932f-7ac07aac48f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "PROJECT_ID = \"633534855904\"\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "job = aiplatform.PipelineJob(display_name = 'sam_test',\n",
    "                             template_path = 'sam_pipe.json',\n",
    "                             enable_caching = True,\n",
    "                             failure_policy = \"slow\",\n",
    "                             project=PROJECT_ID,\n",
    "                             location=REGION,\n",
    "                            )\n",
    "\n",
    "job.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1734cddd-2256-4a24-9565-50425f29bf55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m108"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
